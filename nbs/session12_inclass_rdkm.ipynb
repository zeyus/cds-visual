{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Grid search - iterating over hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:30:49.911274Z",
     "iopub.status.busy": "2022-05-04T07:30:49.910756Z",
     "iopub.status.idle": "2022-05-04T07:30:49.942230Z",
     "shell.execute_reply": "2022-05-04T07:30:49.941282Z",
     "shell.execute_reply.started": "2022-05-04T07:30:49.911221Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# from scikit learn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TensorFlow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# scikeras wrapper\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load and preprocess MNIST data__\n",
    "\n",
    "See Session 7 notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:25:46.040606Z",
     "iopub.status.busy": "2022-05-04T07:25:46.040082Z",
     "iopub.status.idle": "2022-05-04T07:25:46.090351Z",
     "shell.execute_reply": "2022-05-04T07:25:46.089364Z",
     "shell.execute_reply.started": "2022-05-04T07:25:46.040550Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initialise Gridsearch parameters__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our Logistic Regression model as ```pipe```.\n",
    "\n",
    "We the make lists of possible values that can be assigned different values - for this, you should check the documentation over at sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:25:47.359743Z",
     "iopub.status.busy": "2022-05-04T07:25:47.359038Z",
     "iopub.status.idle": "2022-05-04T07:25:47.370202Z",
     "shell.execute_reply": "2022-05-04T07:25:47.368726Z",
     "shell.execute_reply.started": "2022-05-04T07:25:47.359683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise the default model, here given the name 'classifier'\n",
    "pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "\n",
    "\n",
    "# Set tunable parameters for grid search\n",
    "penalties = ['l1', 'l2'] # different regularization parameters\n",
    "C = [1.0, 0.1, 0.01]     # different regularization 'strengths'\n",
    "solvers = ['liblinear']  # different solvers - check all of the sklearn docs\n",
    "\n",
    "# Create parameter grid (a Python dictionary)\n",
    "parameters = dict(classifier__penalty = penalties,  # notice how we use the name 'classifier'\n",
    "                  classifier__C = C,\n",
    "                  classifier__solver = solvers)\n",
    "\n",
    "# Choose which metrics on which we want to optimise\n",
    "scores = ['precision', 'recall', 'f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Iterate over scoring types__\n",
    "\n",
    "For example, we first optimise for the parameters which result in the best weighted precision score; next we optimse for weighted recall; and lastly for weighted-F1. \n",
    "\n",
    "This allows us to inspet the model in a more nuanced way, seeing how different parameters affect performance across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:25:59.086486Z",
     "iopub.status.busy": "2022-05-04T07:25:59.085717Z",
     "iopub.status.idle": "2022-05-04T07:26:19.609086Z",
     "shell.execute_reply": "2022-05-04T07:26:19.607983Z",
     "shell.execute_reply.started": "2022-05-04T07:25:59.086419Z"
    }
   },
   "outputs": [],
   "source": [
    "for score in scores:\n",
    "    print(f\"# Tuning hyper-parameters for {score}\")\n",
    "    print()\n",
    "    \n",
    "    # Initialise Gridsearch with predefined parameters\n",
    "    clf = GridSearchCV(pipe, \n",
    "                       parameters, \n",
    "                       scoring= f\"{score}_weighted\",\n",
    "                       cv=10) # use 10-fold cross-validation\n",
    "    # Fit\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Print best results on training data\n",
    "    print(\"Best parameters set found on training data:\")\n",
    "    # add new lines to separate rows\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on training data:\")\n",
    "    print()\n",
    "    # get all means\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    # get all standard deviations\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    # get parameter combinations\n",
    "    params = clf.cv_results_['params']\n",
    "\n",
    "    # print means, standard deviations , and parameters for all runs\n",
    "    i = 0\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        # 2*standard deviation covers 95% of the spread - check out the 68–95–99.7 rule\n",
    "        print(f\"Run {i}: {round(mean,3)} (SD=±{round(stdev*2, 3)}), using {param}\")\n",
    "        i += 1\n",
    "    print()\n",
    "    \n",
    "    # Print details classification report\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full training set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same general pipeline can be applied to more complex deep learning models, such as those using CNNs or similar.\n",
    "\n",
    "To do this, we have to define our model using ```tf.keras``` in a slightly different way from what we are used to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspect image shapes for input layer size__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:27:16.494482Z",
     "iopub.status.busy": "2022-05-04T07:27:16.493941Z",
     "iopub.status.idle": "2022-05-04T07:27:16.503693Z",
     "shell.execute_reply": "2022-05-04T07:27:16.502499Z",
     "shell.execute_reply.started": "2022-05-04T07:27:16.494424Z"
    }
   },
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define model__\n",
    "\n",
    "We begin by defining our model, almost the same as we normally would. The main difference is that we wrap the whole thign in a function definition, which we've here called ```nn_model```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:37:00.550732Z",
     "iopub.status.busy": "2022-05-04T07:37:00.550211Z",
     "iopub.status.idle": "2022-05-04T07:37:00.561353Z",
     "shell.execute_reply": "2022-05-04T07:37:00.560316Z",
     "shell.execute_reply.started": "2022-05-04T07:37:00.550677Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nn_model(optimizer='adam'):\n",
    "    # create a sequential model\n",
    "    model = Sequential()\n",
    "    # add input layer of 64 nodes and hidden layer of 32, ReLU activation\n",
    "    model.add(Dense(32, \n",
    "                    input_shape=(64,), \n",
    "                    activation=\"relu\"))\n",
    "    # hidden layer of 16 nodes, ReLU activation\n",
    "    model.add(Dense(16, \n",
    "                    activation=\"relu\"))\n",
    "    # classificaiton layer, 10 classes with softmaxa ctivation\n",
    "    model.add(Dense(10, \n",
    "                    activation=\"softmax\")) \n",
    "    # categorical cross-entropy, optimizer defined in function call\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    # return the compiled model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create model for ```sklearn```__\n",
    "\n",
    "We take the predefined neural network model above and run it through ```KerasClassifier```. \n",
    "\n",
    "This returns an object that can be used in the ```sklearn``` pipeline, just like a ```LogisticRegression()``` classifier, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:37:01.152919Z",
     "iopub.status.busy": "2022-05-04T07:37:01.152445Z",
     "iopub.status.idle": "2022-05-04T07:37:01.159678Z",
     "shell.execute_reply": "2022-05-04T07:37:01.158390Z",
     "shell.execute_reply.started": "2022-05-04T07:37:01.152851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=nn_model, # build the model defined in nn_model\n",
    "                        verbose=0)         # set to 1 for verbose output during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define grid search parameters__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our grid search parameters in exactly the same manner as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:37:02.161634Z",
     "iopub.status.busy": "2022-05-04T07:37:02.161161Z",
     "iopub.status.idle": "2022-05-04T07:37:02.169925Z",
     "shell.execute_reply": "2022-05-04T07:37:02.168589Z",
     "shell.execute_reply.started": "2022-05-04T07:37:02.161581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['sgd', 'adam']\n",
    "# range of epochs to run\n",
    "epochs = [5, 10]\n",
    "# variable batch sizes\n",
    "batches = [5, 10]\n",
    "\n",
    "# create search grid\n",
    "param_grid = dict(optimizer=optimizers, \n",
    "                  epochs=epochs, \n",
    "                  batch_size=batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initialise Gridsearch with model and parameter grid__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run grid search using these parameters and the neural network model that we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:37:03.207961Z",
     "iopub.status.busy": "2022-05-04T07:37:03.207472Z",
     "iopub.status.idle": "2022-05-04T07:37:03.215053Z",
     "shell.execute_reply": "2022-05-04T07:37:03.213846Z",
     "shell.execute_reply.started": "2022-05-04T07:37:03.207907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1,    # number of CPU cores to use: -1 means use all available\n",
    "                    cv=5,         # 5-fold cross validation\n",
    "                    scoring='accuracy',\n",
    "                    verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fit to the data and labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:37:07.974515Z",
     "iopub.status.busy": "2022-05-04T07:37:07.973986Z",
     "iopub.status.idle": "2022-05-04T07:37:22.069819Z",
     "shell.execute_reply": "2022-05-04T07:37:22.068679Z",
     "shell.execute_reply.started": "2022-05-04T07:37:07.974460Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Print best results__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell prints the parameters which return the best possible model from all combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:37:28.170707Z",
     "iopub.status.busy": "2022-05-04T07:37:28.170184Z",
     "iopub.status.idle": "2022-05-04T07:37:28.178289Z",
     "shell.execute_reply": "2022-05-04T07:37:28.177311Z",
     "shell.execute_reply.started": "2022-05-04T07:37:28.170652Z"
    }
   },
   "outputs": [],
   "source": [
    "# print best results, rounding values to 3 decimal places\n",
    "print(f\"Best run: {round(grid_result.best_score_,3)} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show all runs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect all runs, in order to see if there are general tendencies or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:37:30.648754Z",
     "iopub.status.busy": "2022-05-04T07:37:30.648236Z",
     "iopub.status.idle": "2022-05-04T07:37:30.659933Z",
     "shell.execute_reply": "2022-05-04T07:37:30.658840Z",
     "shell.execute_reply.started": "2022-05-04T07:37:30.648698Z"
    }
   },
   "outputs": [],
   "source": [
    "# get all means\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "# get all standard deviations\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "# get parameter combinations\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "# print means, standard deviations, and parameters for all runs\n",
    "i = 0\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Run {i}: {round(mean,3)} (SD=±{round(2*stdev, 3)}), using {param}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualise feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful task in the context of visual anlaytics and Convolutional Neural Networks is the idea of visualising feature maps. In other words, can we see what a model is actually 'looking' at?\n",
    "\n",
    "One way to do this is to use an activation heatmap, to show which areas draw the most 'focus' from a model when classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:38:23.367733Z",
     "iopub.status.busy": "2022-05-04T07:38:23.367212Z",
     "iopub.status.idle": "2022-05-04T07:38:23.866554Z",
     "shell.execute_reply": "2022-05-04T07:38:23.865924Z",
     "shell.execute_reply.started": "2022-05-04T07:38:23.367677Z"
    }
   },
   "outputs": [],
   "source": [
    "# path tools\n",
    "import sys,os\n",
    "sys.path.append(\"..\")\n",
    "# neural networks with numpy\n",
    "from utils.imutils import jimshow\n",
    "\n",
    "# image processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import (preprocess_input,\n",
    "                                                 decode_predictions,\n",
    "                                                 VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import VGG16 model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're choosing here to work with VGG16 but the same principle can be applied to other pretrained models, or your own models. The only thing that will need to be changed is the name of the final convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:38:27.500519Z",
     "iopub.status.busy": "2022-05-04T07:38:27.500001Z",
     "iopub.status.idle": "2022-05-04T07:38:38.604692Z",
     "shell.execute_reply": "2022-05-04T07:38:38.604022Z",
     "shell.execute_reply.started": "2022-05-04T07:38:27.500463Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load image__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to load an image - look back to the last few weeks to check up on how to load images with ```tf.keras```. Remember that your image needs to be the same dimensions as the pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:40:04.278636Z",
     "iopub.status.busy": "2022-05-04T07:40:04.277971Z",
     "iopub.status.idle": "2022-05-04T07:40:04.317805Z",
     "shell.execute_reply": "2022-05-04T07:40:04.317186Z",
     "shell.execute_reply.started": "2022-05-04T07:40:04.278577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file path - hard coded paths are bad practice!\n",
    "filepath = os.path.join(\"..\", \"..\", \"CDS-VIS\", \"img\", \"florence.jpg\")\n",
    "\n",
    "# load image using tf.keras\n",
    "img = image.load_img(filepath, target_size=(224, 224))\n",
    "\n",
    "# display image\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocess and predict__\n",
    "\n",
    "See Session 10 notebook for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:40:08.824616Z",
     "iopub.status.busy": "2022-05-04T07:40:08.824094Z",
     "iopub.status.idle": "2022-05-04T07:40:09.541205Z",
     "shell.execute_reply": "2022-05-04T07:40:09.540198Z",
     "shell.execute_reply.started": "2022-05-04T07:40:08.824559Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert image to array\n",
    "x = image.img_to_array(img)\n",
    "# convert to rank 4 tensor\n",
    "x = np.expand_dims(x, axis=0)\n",
    "# preprocess to be in line with VGG16 data \n",
    "x = preprocess_input(x)\n",
    "\n",
    "# make predictions\n",
    "preds = model.predict(x)\n",
    "decode_predictions(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create activation heatmap for final layer__\n",
    "\n",
    "In order to visualise the layers, we use something called **[Gradient-weighted Class Activation Mapping (Grad-CAM)](https://arxiv.org/pdf/1610.02391.pdf)**. \n",
    "\n",
    "Essentially, we make use of the gradients in the final layer to highlight which regions are particularly informative when predicting a given class.\n",
    "\n",
    "The code here is a little complicated to follow - you can find more info [here](https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/) and [here](https://www.tensorflow.org/guide/autodiff).\n",
    "\n",
    "The code below has been modified from Keras documentation, found [here](https://keras.io/examples/vision/grad_cam/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:40:16.363836Z",
     "iopub.status.busy": "2022-05-04T07:40:16.363321Z",
     "iopub.status.idle": "2022-05-04T07:40:17.061688Z",
     "shell.execute_reply": "2022-05-04T07:40:17.060939Z",
     "shell.execute_reply.started": "2022-05-04T07:40:16.363781Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    # make sure the name here corresponds to the final conv layer in your network\n",
    "    last_conv_layer = model.get_layer('block5_conv3')\n",
    "    \n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions    \n",
    "    iterate = tf.keras.models.Model([model.inputs], \n",
    "                                    [model.output, last_conv_layer.output])\n",
    "    \n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    model_out, last_conv_layer = iterate(x)\n",
    "    class_out = model_out[:, np.argmax(model_out[0])]\n",
    "    \n",
    "    # This is the gradient of the output neuron of the last conv layer\n",
    "    grads = tape.gradient(class_out, \n",
    "                          last_conv_layer)\n",
    "    # Vector of mean intensity of the gradient over a specific feature map channel\n",
    "    pooled_grads = K.mean(grads, \n",
    "                          axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:40:23.802519Z",
     "iopub.status.busy": "2022-05-04T07:40:23.801820Z",
     "iopub.status.idle": "2022-05-04T07:40:23.809881Z",
     "shell.execute_reply": "2022-05-04T07:40:23.808822Z",
     "shell.execute_reply.started": "2022-05-04T07:40:23.802462Z"
    }
   },
   "outputs": [],
   "source": [
    "# We multiply each channel in the feature map array\n",
    "# by \"how important this channel is\" with regard to the top predicted class\n",
    "# then sum all the channels to obtain the heatmap class activation\n",
    "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), \n",
    "                         axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Simple heatmap using matplotlib__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a very simple heatmap showing where 'attention' is focused in the final layer when predicting our input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:40:27.719653Z",
     "iopub.status.busy": "2022-05-04T07:40:27.719128Z",
     "iopub.status.idle": "2022-05-04T07:40:27.847575Z",
     "shell.execute_reply": "2022-05-04T07:40:27.846917Z",
     "shell.execute_reply.started": "2022-05-04T07:40:27.719597Z"
    }
   },
   "outputs": [],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "heatmap = heatmap.reshape((14, 14))\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Better visualisation with ```OpenCV```__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:40:39.659719Z",
     "iopub.status.busy": "2022-05-04T07:40:39.659040Z",
     "iopub.status.idle": "2022-05-04T07:40:39.701801Z",
     "shell.execute_reply": "2022-05-04T07:40:39.701101Z",
     "shell.execute_reply.started": "2022-05-04T07:40:39.659661Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the original image\n",
    "img = cv2.imread(filepath)\n",
    "\n",
    "# heatmap should be semi transparent\n",
    "intensity = 0.5\n",
    "\n",
    "# resize the heatmap to be the original dimensions of the input \n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# https://docs.opencv.org/master/d3/d50/group__imgproc__colormap.html\n",
    "heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "# multiply heatmap by intensity and 'add' this on top of the original image\n",
    "superimposed = (heatmap * intensity) + img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write file to output__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a new folder called maps_out where I save the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-04T07:40:51.362071Z",
     "iopub.status.busy": "2022-05-04T07:40:51.361535Z",
     "iopub.status.idle": "2022-05-04T07:40:51.389279Z",
     "shell.execute_reply": "2022-05-04T07:40:51.388403Z",
     "shell.execute_reply.started": "2022-05-04T07:40:51.362014Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2.imwrite(\"florence.jpg\", superimposed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
