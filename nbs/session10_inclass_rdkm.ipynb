{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 10 - Image search with VGG16 and K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base tools\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "\n",
    "# data analysis\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from tqdm import notebook\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import (load_img, \n",
    "                                                  img_to_array)\n",
    "from tensorflow.keras.applications.vgg16 import (VGG16, \n",
    "                                                 preprocess_input)\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What kind of preprocessing am I doing here? Why do you think I'm doing it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_path, model):\n",
    "    \"\"\"\n",
    "    Extract features from image data using pretrained model (e.g. VGG16)\n",
    "    \"\"\"\n",
    "    # Define input image shape - remember we need to reshape\n",
    "    input_shape = (224, 224, 3)\n",
    "    # load image from file path\n",
    "    img = load_img(img_path, target_size=(input_shape[0], \n",
    "                                          input_shape[1]))\n",
    "    # convert to array\n",
    "    img_array = img_to_array(img)\n",
    "    # expand to fit dimensions\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "    # preprocess image - see last week's notebook\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    # use the predict function to create feature representation\n",
    "    features = model.predict(preprocessed_img)\n",
    "    # flatten\n",
    "    flattened_features = features.flatten()\n",
    "    # normalise features\n",
    "    normalized_features = flattened_features / norm(features)\n",
    "    return flattened_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', \n",
    "              include_top=False,\n",
    "              pooling='avg',\n",
    "              input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features('../data/img/florence.jpg', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the datasets\n",
    "root_dir = os.path.join(\"..\")\n",
    "filenames = [root_dir + \"/\" + name for name in sorted(os.listdir(root_dir))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract features for each image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "for i in notebook.tqdm(range(len(filenames))):\n",
    "    feature_list.append(extract_features(filenames[i], model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our *database* of extracted embeddings, we can then use K-Nearest Neighbours to find similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neighbors = NearestNeighbors(n_neighbors=10, \n",
    "                             algorithm='brute',\n",
    "                             metric='cosine').fit(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate nearest neighbours for target__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = neighbors.kneighbors([feature_list[250]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save indices, print data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for i in range(1,6):\n",
    "    print(distances[0][i], indices[0][i])\n",
    "    idxs.append(indices[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot target image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(filenames[250]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot close images__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mpimg.imread(filenames[251]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot target and top 3 closest together__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt target\n",
    "plt.imshow(mpimg.imread(filenames[251]))\n",
    "\n",
    "# plot 3 most similar\n",
    "f, axarr = plt.subplots(1,3)\n",
    "axarr[0].imshow(mpimg.imread(filenames[idxs[0]]))\n",
    "axarr[1].imshow(mpimg.imread(filenames[idxs[1]]))\n",
    "axarr[2].imshow(mpimg.imread(filenames[idxs[2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple style transfer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load a quick style transfer model from TF Hub__\n",
    "\n",
    "You can find more details [here](https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load TF-Hub module.\n",
    "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
    "hub_module = hub.load(hub_handle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load the content image and the style image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = st_load(\"../data/img/florence.jpg\")\n",
    "style_image = st_load(\"../data/img/starry_night.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Process using the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = hub_module(content_image, style_image)\n",
    "stylized_image = outputs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Show content, style, and stylized image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n([content_image, style_image, stylized_image], \n",
    "       titles=['Original content image', 'Style image', 'Stylized image'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 \n",
    "- Run this same pipeline on the Indo Fashion dataset. How does it perform?\n",
    "\n",
    "### Task 2\n",
    "- Take the code in this notebook and turn it into a Python script. You can then add this to the repo for your Assignment 1 solution for creating doing image search\n",
    "  - I.e. your Assignment 1 repo would contain both code for image search using colour histograms *and* for image search using a pretrained CNN.\n",
    "\n",
    "### Task 3 \n",
    "- Continue working on Assignment 3 in-class just now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
